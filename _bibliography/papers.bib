---
---

@inproceedings{sundaresan2025emnlp_browsing,
  abbr    = {EMNLP},
  bibtex_show = {true},
  title   = {Subjective Behaviors and Preferences in LLM: Language of Browsing},
  author  = {Sundaresan, Sai and Chopra, Harshita and Sinha, Atanu R. and Goswami, Koustava and Naidu, Nagasai Saketh and Karan, Raghav and Anushka, N.},
  booktitle = {The 30th Conference on Empirical Methods in Natural Language Processing},
  year    = {2025},
  arxiv   = {2508.15474},
  html    = {https://arxiv.org/abs/2508.15474},
  abstract = {A Large Language Model (LLM) offers versatility across domains and tasks, purportedly benefiting users with a wide variety of behaviors and preferences. We question this perception about an LLM when users have inherently subjective behaviors and preferences, as seen in their ubiquitous and idiosyncratic browsing of websites or apps. The sequential behavior logs of pages, thus generated, form something akin to each user's self-constructed "language", albeit without the structure and grammar imbued in natural languages. We ask: (i) Can a small LM represent the "language of browsing" better than a large LM? (ii) Can an LM with a single set of parameters (or, single LM) adequately capture myriad users' heterogeneous, subjective behaviors and preferences? (iii) Can a single LM with high average performance, yield low variance in performance to make alignment good at user level? We introduce clusterwise LM training, HeTLM (Heterogeneity aware Training of Language Model), appropriate for subjective behaviors. We find that (i) a small LM trained using a page-level tokenizer outperforms large pretrained or finetuned LMs; (ii) HeTLM with heterogeneous cluster specific set of parameters outperforms a single LM of the same family, controlling for the number of parameters; and (iii) a higher mean and a lower variance in generation ensues, implying improved alignment.},
  selected= {true}
}

@inproceedings{agarwal2025sigmod_cachecraft,
  abbr    = {SIGMOD},
  bibtex_show = {true},
  title   = {Cache-Craft: Managing Chunk-Caches for Efficient Retrieval-Augmented Generation},
  author  = {Agarwal*, Shubham and Sundaresan*, Sai and Mitra, Subrata and Mahapatra, Debabrata and Gupta, Archit and Sharma, Rounak and Kapu, Nirmal Joshua and Yu, Tong and Saini, Shiv},
  booktitle = {The 44th ACM SIGMOD International Conference on Management of Data},
  year    = {2025},
  arxiv   = {2502.15734},
  html    = {https://arxiv.org/abs/2502.15734},
  abstract = {Retrieval-Augmented Generation (RAG) is often used with Large Language Models (LLMs) to infuse domain knowledge or user-specific information. In RAG, given a user query, a retriever extracts chunks of relevant text from a knowledge base. These chunks are sent to an LLM as part of the input prompt. Typically, any given chunk is repeatedly retrieved across user questions. However, currently, for every question, attention-layers in LLMs fully compute the key values (KVs) repeatedly for the input chunks, as state-of-the-art methods cannot reuse KV-caches when chunks appear at arbitrary locations with arbitrary contexts. Naive reuse leads to output quality degradation. This leads to potentially redundant computations on expensive GPUs and increases latency. In this work, we propose Cache-Craft, a system for managing and reusing precomputed KVs corresponding to the text chunks (we call chunk-caches) in RAG-based systems. We present how to identify chunk-caches that are reusable, how to efficiently perform a small fraction of recomputation to fix the cache to maintain output quality, and how to efficiently store and evict chunk-caches in the hardware for maximizing reuse while masking any overheads. With real production workloads as well as synthetic datasets, we show that Cache-Craft reduces redundant computation by 51% over SOTA prefix-caching and 75% over full recomputation. Additionally, with continuous batching on a real production workload, we get a 1.6X speed up in throughput and a 2X reduction in end-to-end response latency over prefix-caching while maintaining quality, for both the LLaMA-3-8B and LLaMA-3-70B models.},
  selected= {true}
}

@inproceedings{sundaresan2024orsi_clearance_sale,
  abbr    = {ORSI},
  bibtex_show = {false},
  title   = {Clearance Sale Models under Competition},
  author  = {Sundaresan, Sai and Abraham, Anand},
  booktitle = {The International Conference on Trends in Business Analytics & Management},
  year    = {2024},
  html    = {https://www.som.iitb.ac.in/bams-orsi-2024/publications_and_awards/},
  abs_only = {Retailers frequently face challenges in demand forecasting which can lead to excess inventory. This is often addressed through clearance sales where the inventory is sold at a markdown price. In these situations, the presence of competition can largely alter the optimal strategy for a retailer. This study proposes a two-period model that determines the optimal initial order quantity for a retailer in the presence of competition. Our model highlights how competition dynamics affect revenue, demonstrating significant gain over the traditional Newsvendor model in limited clearance scenarios. Furthermore, we extend the model to study the impact of risk aversion on these decisions.}
}

@inproceedings{sundaresan2024iclst_hidden_city,
  abbr    = {ICLST},
  bibtex_show = {false},
  title   = {Single Resource Capacity Control Model for Hidden City Ticketing},
  author  = {Sundaresan, Sai and Abraham, Anand},
  booktitle = {The International Conference on Logistics, Supply Chain and Transportation},
  year    = {2024},
  html    = {https://link.springer.com/book/9789819506026},
  abs_only = {Skip-Lagging is a practice where a strategic customer takes advantage of a situation where a connecting flight through an intermediate city is cheaper than a direct flight to the city. In such a case, the customer wishing to travel to an intermediate city will purchase a connecting flight through the city and deplane at the connection point. In this paper, we try to model the effect of skip-lagging behaviour on airline revenue under a single resource capacity control scenario. Our Dual Booking limit considers separate booking limits for the direct and connecting flight segments while determining the optimal seat allocation. We demonstrate that our model achieves a significant revenue increase compared to the traditional single booking limit model. Moreover, we show our model incentivizes customers to refrain from engaging in skip-lagging practices. We also analyze the solution space of the model to develop an efficient Integer Linear Programming formulation for the seat allocation problem. Furthermore, we extend the model to scenarios where multiple Skip-Lagging opportunities are present for strategic customers.}
}

@inproceedings{bhogale2023interspeech_vistaar,
  abbr    = {INTERSPEECH},
  bibtex_show = {true},
  title   = {Vistaar: Diverse Benchmarks and Training Sets for Indian Language ASR},
  author  = {Bhogale*, Kaushal Santosh and Sundaresan*, Sai and Raman, Abhigyan and Javed, Tahir and Khapra, Mitesh M. and Kumar, Pratyush},
  booktitle = {The 24th INTERSPEECH Conference},
  year    = {2023},
  arxiv   = {2305.15386},
  html    = {https://arxiv.org/abs/2305.15386},
  abstract = {Improving ASR systems is necessary to make new LLM-based use-cases accessible to people across the globe. In this paper, we focus on Indian languages, and make the case that diverse benchmarks are required to evaluate and improve ASR systems for Indian languages. To address this, we collate Vistaar as a set of 59 benchmarks across various language and domain combinations, on which we evaluate 3 publicly available ASR systems and 2 commercial systems. We also train IndicWhisper models by fine-tuning the Whisper models on publicly available training datasets across 12 Indian languages totalling to 10.7K hours. We show that IndicWhisper significantly improves on considered ASR systems on the Vistaar benchmark. Indeed, IndicWhisper has the lowest WER in 39 out of the 59 benchmarks, with an average reduction of 4.1 WER. We open-source all datasets, code and models.}
}

@inproceedings{javed2023interspeech_svarah,
  abbr    = {INTERSPEECH},
  bibtex_show = {true},
  title   = {Svarah: Evaluating English ASR Systems on Indian Accents},
  author  = {Javed, Tahir and Joshi, Sakshi and Nagarajan, Vignesh and Sundaresan, Sai and Nawale, Janki and Raman, Abhigyan and Bhogale, Kaushal and Kumar, Pratyush and Khapra, Mitesh M.},
  booktitle = {The 24th INTERSPEECH Conference},
  year    = {2023},
  arxiv   = {2305.15760},
  html    = {https://arxiv.org/abs/2305.15760},
  abstract = {India is the second largest English-speaking country in the world with a speaker base of roughly 130 million. Thus, it is imperative that automatic speech recognition (ASR) systems for English should be evaluated on Indian accents. Unfortunately, Indian speakers find a very poor representation in existing English ASR benchmarks such as LibriSpeech, Switchboard, Speech Accent Archive, etc. In this work, we address this gap by creating Svarah, a benchmark that contains 9.6 hours of transcribed English audio from 117 speakers across 65 geographic locations throughout India, resulting in a diverse range of accents. Svarah comprises both read speech and spontaneous conversational data, covering various domains, such as history, culture, tourism, etc., ensuring a diverse vocabulary. We evaluate 6 open source ASR models and 2 commercial ASR systems on Svarah and show that there is clear scope for improvement on Indian accents. Svarah as well as all our code will be publicly available.}
}


